{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(500)                                                        # Number of neurons\n",
    "a = 10                                                              # Parameters for propensity function\n",
    "theta_stim = 90                                                     # Angle to stimulate at \n",
    "n_test_angles = 100                                                 # Number of angles to use to test for preferred orientation\n",
    "vars = np.random.lognormal(2, 0.6, N)                               # Width of each neuron's tuning curve\n",
    "learning_rate = 0.01                                                # Learning rate\n",
    "\n",
    "n_norm_per_day = 5                                                  # How many times to normalise the weights per day     \n",
    "n_steps_per_norm = 30                                               # How many orientation stimuli per day\n",
    "init_steps = 300                                                    # Number of trials to run to settle to a baseline\n",
    "\n",
    "hebb_scaling = 0.3                                                  # Scaling of Hebbian component\n",
    "rand_scaling = 1                                                    # Scaling of random component \n",
    "\n",
    "n_days_deprv = 28                                                   # Number of days of stripe-rearing\n",
    "n_days_recovery = 1000                                              # Number of days of recovery\n",
    "n_steps_deprv = n_days_deprv * n_steps_per_norm * n_norm_per_day  \n",
    "n_steps_recovery = n_days_recovery * n_steps_per_norm * n_norm_per_day\n",
    "\n",
    "n_repeats = 10                                                      # Number of repeats\n",
    "\n",
    "POS_pre_dep = []; POS_post_dep = []; POS_post_rec = []\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "\n",
    "    W_init = initialise_W(N, vars)                                                                       # Initialise weights \n",
    "    W_baseline = prerun(W_init, theta_stim, a, 0.4, 1, learning_rate, n_steps_per_norm, init_steps)      # Settle to a baseline\n",
    "\n",
    "    \"\"\" Stripe-rearing \"\"\" \n",
    "\n",
    "    POs = []; W = np.zeros((N, N, n_steps_deprv+1)); W[:, :, 0] = W_baseline\n",
    "    \n",
    "    for t in tqdm(range(n_steps_deprv)):\n",
    "        W_old = W[:, :, t]\n",
    "        H = single_hebbian_component(N, W_old, theta_stim, type='stripe_rearing')\n",
    "        eta = np.random.randn(N, N)\n",
    "        prop_function = propensity(W_old, a)\n",
    "        hebb = hebb_scaling * H * prop_function\n",
    "        rand = rand_scaling * eta * prop_function\n",
    "        W_new = W_old + (hebb + rand) * learning_rate\n",
    "        if t % n_steps_per_norm == 0: normalisation(W_new)\n",
    "        W[:, :, t+1] = W_new\n",
    "\n",
    "        if t==0: PO_pre_dep = get_preferred_orientations(N, W_new, n_angles=n_test_angles)\n",
    "        if t==n_steps_deprv-1: PO_post_dep = get_preferred_orientations(N, W_new, n_angles=n_test_angles)\n",
    "        \n",
    "    W_post_deprivation = W[:, :, -1]\n",
    "\n",
    "    \"\"\" Recovery \"\"\" \n",
    "\n",
    "    POs = []; W = np.zeros((N, N, n_steps_recovery+1)); W[:, :, 0] = W_post_deprivation\n",
    "\n",
    "\n",
    "    for t in tqdm(range(n_steps_recovery)):\n",
    "        W_old = W[:, :, t]\n",
    "        H = single_hebbian_component(N, W_old, theta_stim, type='baseline')\n",
    "        eta = np.random.randn(N, N)\n",
    "        prop_function = propensity(W_old, a)\n",
    "        hebb = hebb_scaling * H * prop_function\n",
    "        rand = rand_scaling * eta * prop_function\n",
    "        W_new = W_old + (hebb + rand) * learning_rate\n",
    "        if t % n_steps_per_norm == 0: normalisation(W_new)\n",
    "        W[:, :, t+1] = W_new\n",
    "\n",
    "        if t==n_steps_recovery-1: PO_post_rec = get_preferred_orientations(N, W_new, n_angles=n_test_angles)\n",
    "\n",
    "    POS_pre_dep.append(PO_pre_dep); POS_post_dep.append(PO_post_dep); POS_post_rec.append(PO_post_rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rPOS_pre_dep = [np.abs(PO_pre_dep - theta_stim) for PO_pre_dep in POS_pre_dep]\n",
    "rPOS_post_dep = [np.abs(PO_post_dep - theta_stim) for PO_post_dep in POS_post_dep]\n",
    "rPOS_post_rec = [np.abs(PO_post_rec - theta_stim) for PO_post_rec in POS_post_rec]\n",
    "convergences_during_deprivation = [rPO_pre_dep - rPO_post_dep for rPO_pre_dep, rPO_post_dep in zip(rPOS_pre_dep, rPOS_post_dep)]\n",
    "convergences_during_recovery = [rPO_post_dep - rPO_post_rec for rPO_post_dep, rPO_post_rec in zip(rPOS_post_dep, rPOS_post_rec)]\n",
    "\n",
    "median_convergence_during_deprivation_28 = np.mean(np.median(convergences_during_deprivation, axis=1))\n",
    "median_convergence_during_recovery_1000 = np.mean(np.median(convergences_during_recovery, axis=1))\n",
    "\n",
    "std_convergence_during_deprivation_28 = np.std(np.median(convergences_during_deprivation, axis=1))\n",
    "std_convergence_during_recovery_1000 = np.std(np.median(convergences_during_recovery, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp. Fig. 3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supp_fig_3k(POS_pre_dep, POS_post_dep, POS_post_rec):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 3), dpi=150)\n",
    "\n",
    "    nbins = 50\n",
    "    c1 = 'firebrick'; c2 = 'coral'; c3 = 'goldenrod'\n",
    "\n",
    "    init_distribution = np.concatenate(POS_pre_dep, axis=0)\n",
    "    sr_distribution = np.concatenate(POS_post_dep, axis=0)\n",
    "    rec_distribution = np.concatenate(POS_post_rec, axis=0)\n",
    "\n",
    "    ax[0].hist(init_distribution, bins=nbins, histtype='stepfilled', color=c1, alpha=0.5)\n",
    "    ax[1].hist(sr_distribution, bins=nbins, histtype='stepfilled', color=c2, alpha=0.5)\n",
    "    ax[2].hist(rec_distribution, bins=nbins, histtype='stepfilled', color=c3, alpha=0.5)\n",
    "\n",
    "    ax[0].set_ylabel('frequency', labelpad=20)\n",
    "    ax[0].set_xlabel('PO initial'); ax[1].set_xlabel('PO post deprivation (28 days)'); ax[2].set_xlabel('PO post recovery (1000 days)')\n",
    "\n",
    "    dict = {ax[0]: init_distribution, ax[1]: sr_distribution, ax[2]: rec_distribution}\n",
    "\n",
    "    for a, data in dict.items():\n",
    "        result = kstest(data, 'uniform', args=(0, 180))\n",
    "        if result.pvalue < 0.05: a.text(0.4, 0.2, f\"D: {result.statistic:.3f}\\np: {result.pvalue:.3f}\", transform=a.transAxes, fontsize=8, color='red')\n",
    "        else: a.text(0.4, 0.2, f\"D: {result.statistic:.3f}\\np: {result.pvalue:.3f}\", transform=a.transAxes, fontsize=8, color='black')\n",
    "\n",
    "    for ax in ax: \n",
    "        ax.set_xlim(0, 180); ax.set_xticks([0, 90, 180]); ax.set_yticks([])\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig. 3l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_recovery(total_days):\n",
    "\n",
    "    n_steps_recovery = n_steps_per_norm * n_norm_per_day * total_days\n",
    "    n_steps_deprivation = n_steps_per_norm * n_norm_per_day * 28\n",
    "\n",
    "    # Initialise weights for stripe-rearing \n",
    "    W_init = initialise_W(N, vars)                                                                                         \n",
    "    W_baseline = prerun(W_init, theta_stim, a, 0.4, 1, learning_rate, n_steps_per_norm, init_steps)                        \n",
    "    W = np.zeros((N, N, n_steps_deprivation+1)); W[:, :, 0] = W_baseline\n",
    "    \n",
    "    # Stripe rearing period \n",
    "    for t in range(n_steps_deprivation):\n",
    "        W_old = W[:, :, t]\n",
    "        H = single_hebbian_component(N, W_old, theta_stim, type='stripe_rearing')\n",
    "        eta = np.random.randn(N, N)\n",
    "        prop_function = propensity(W_old, a)\n",
    "        hebb = hebb_scaling * H * prop_function\n",
    "        rand = rand_scaling * eta * prop_function\n",
    "        W_new = W_old + (hebb + rand) * learning_rate\n",
    "        if t % n_steps_per_norm == 0: normalisation(W_new)\n",
    "        W[:, :, t+1] = W_new\n",
    "\n",
    "        if t==0: PO_pre_dep = get_preferred_orientations(N, W_new, n_angles=n_test_angles)\n",
    "        if t==n_steps_deprivation-1: PO_post_dep = get_preferred_orientations(N, W_new, n_angles=n_test_angles)\n",
    "        \n",
    "    W_post_deprivation = W[:, :, -1]\n",
    "\n",
    "    rPO_pre_dep = np.abs(PO_pre_dep - theta_stim); rPO_post_dep = np.abs(PO_post_dep - theta_stim)\n",
    "    convergence_during_deprivation = rPO_pre_dep - rPO_post_dep\n",
    "\n",
    "    # Recovery period\n",
    "    W_rec_step = np.zeros((N, N, n_steps_recovery+1)); \n",
    "    W_rec_day = np.zeros((N, N, total_days+1)); \n",
    "    correlations = np.zeros(total_days+1)\n",
    "    medians_recovery = np.zeros(total_days+1)\n",
    "    median_deprivation = np.median(convergence_during_deprivation)\n",
    "\n",
    "    POs_rec = np.zeros((N, total_days+1)); POs_rec[:, 0] = get_preferred_orientations(N, W_post_deprivation, n_angles=n_test_angles)\n",
    "    \n",
    "    # Initialise weights for recovery as those after stripe-rearing \n",
    "    W_rec_step[:, :, 0] = W_post_deprivation\n",
    "\n",
    "    for t in range(n_steps_recovery):\n",
    "        W_new = evolve_weights(N, W_rec_step[:, :, t], t, 'baseline', theta_stim, a, learning_rate, hebb_scaling, rand_scaling, n_steps_per_norm)\n",
    "        W_rec_step[:, :, t+1] = W_new\n",
    "        if t % (n_steps_per_norm * n_norm_per_day) == 0: \n",
    "            POs_rec[:, int(t/(n_steps_per_norm * n_norm_per_day))+1] = get_preferred_orientations(N, W_new, n_angles=n_test_angles)      \n",
    "            W_rec_day[:, :, int(t/(n_steps_per_norm * n_norm_per_day))+1] = W_new\n",
    "\n",
    "    convergence_during_recoveries_after_this_sr_period = [np.abs(PO_post_dep - theta_stim) - np.abs(PO - theta_stim) for PO in POs_rec.T]\n",
    "\n",
    "    for rec_day in range(total_days+1):\n",
    "        correlations[rec_day] = np.corrcoef(convergence_during_deprivation, convergence_during_recoveries_after_this_sr_period[rec_day])[0, 1] \n",
    "        medians_recovery[rec_day] = np.median(convergence_during_recoveries_after_this_sr_period[rec_day])\n",
    "\n",
    "    return correlations, medians_recovery, median_deprivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_days = 80\n",
    "repeats = 50\n",
    "\n",
    "all_correlations = np.zeros((repeats, total_days+1))\n",
    "all_medians_recovery = np.zeros((repeats, total_days+1))\n",
    "medians_deprivation = np.zeros(repeats)\n",
    "\n",
    "for repeat in tqdm(range(repeats)):\n",
    "    all_correlations[repeat, :], all_medians_recovery[repeat, :], medians_deprivation[repeat] = get_correlation_recovery(total_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_3l(all_medians_recovery, median_convergence_during_recovery_1000, std_convergence_during_recovery_1000, median_convergence_during_deprivation_28):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), dpi=180)\n",
    "    ax2 = ax.twinx()\n",
    "    end_point = 95\n",
    "\n",
    "    ax.plot(np.mean(all_medians_recovery,  axis=0), label='Median')\n",
    "    ax.fill_between(np.arange(total_days+1), np.mean(all_medians_recovery,  axis=0) - np.std(all_medians_recovery,  axis=0), np.mean(all_medians_recovery,  axis=0) + np.std(all_medians_recovery,  axis=0), alpha=0.2)\n",
    "\n",
    "    ax.set_xlabel('recovery [days]')\n",
    "    ax.set_ylabel(r'convergence during recovery $[\\degree]$', color='steelblue', labelpad=10)\n",
    "\n",
    "    ax.scatter(end_point, median_convergence_during_recovery_1000, c='steelblue', s=20, clip_on=False)\n",
    "    ax.errorbar(end_point, median_convergence_during_recovery_1000, yerr=std_convergence_during_recovery_1000, fmt='o', c='steelblue', capsize=2, capthick=1, elinewidth=1, markersize=4, zorder=2, clip_on=False)\n",
    "\n",
    "    ax2.axhline(median_convergence_during_deprivation_28, c='darkgreen', ls='--', lw=1)\n",
    "\n",
    "    ax.set_xticks([0, 20, 40, 60, 80, end_point])\n",
    "    ax.set_xticklabels([0, 20, 40, 60, 80, '1000'])\n",
    "\n",
    "    ax.set_ylim(-4.2, 0.2)\n",
    "    ax2.set_ylim(0, 4.2)\n",
    "    ax.set_xlim(0, end_point+5)\n",
    "    ax.invert_yaxis()\n",
    "    polygon = Polygon(np.array([[85, -5], [90, -5], [90, 0.5], [85, 0.5]]), closed=False, color='white', lw=0.5, clip_on=False, zorder=3)\n",
    "    ax2.add_patch(polygon)\n",
    "    ax2.spines['right'].set_visible(True)\n",
    "    ax2.set_ylabel(r'convergence during deprivation  $[\\degree]$', color='darkgreen', labelpad=10)\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drift 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd7650aea0822ae05dcf176e2ed0901be08f4fc732776693377ded011ec53f5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
